---
title: |
  ![](images/svm.png){width=20%} 
  Partie 2 sur R
author: 
- Clovis Deletre
- Charles Vitry
date:
output:
  rmarkdown::html_document:
    theme: cerulean
    number_sections: no
    toc: yes
    toc_depth: 5
    toc_float: true
---
<style type="text/css">

body{ /* Normal  */
      font-size: 20px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 55px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 38px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 28px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 35px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install for export in pdf file
#tinytex::install_tinytex()
```
<br> </br>





<br> </br>


# Introduction
<br> </br>
Import des deux jeux de données | format csv 
```{r}
train <-
  read.csv(file = "churner_train_data_set.csv", header = TRUE, sep = ",")
#head(train)

test <- read.csv(file = "churner_test_data_set.csv", header = TRUE, sep =
                   ",")
#head(test)
```
<br> </br>
Création variables
```{r}
train$groupe <- ifelse(train$churner == 1 , TRUE, FALSE)
test$groupe <- ifelse(test$churner == 1 , TRUE, FALSE)
```
<br> </br>    
<br> </br>
<br> </br>

# ACP sur Jeu d'entrainement
<br> </br>

scale.unit: pour choisir de réduire ou non les variables
ncp: le nombre de dimensions à garder dans les résultats
graph: pour choisir de faire apparaître les graphiques ou non 
```{r,fig.show='hide',warning=FALSE}
library(FactoMineR)
library("factoextra")

#Test d'une ACP en prenant churner en variable qualitative
res.pca = PCA(
  train[, 2:133],
  scale.unit = TRUE,
  quali.sup = 132 ,
  ncp = 131,
  graph = T
)

```
<br> </br>

## Création de l'ACP

```{r,fig.show='hide',warning=FALSE}
library(FactoMineR)
library("factoextra")

#ACP en excluant la collone d'identification de l'individu ainsi que la variable à expliquée.
#res.pca = PCA(train[,2:132], scale.unit=TRUE, ncp=131, graph=T)


#mauvaise méthode, faire l'ACP sur les données à tester n'a aucun sens, 
#en effet, pour estimer le prix d'un contrat, on ne peut pas attendre que 5000 clients attendent l'estimation de leur contrat
#il faut projeter les nouvelles données avec l'ACP effectué sur le jeu d'entrainement.
#res.pca.test = PCA(test[,2:132], scale.unit=TRUE, ncp=131, graph=T)


library(ade4)
res.pca <- dudi.pca(train[,2:132],
                    scannf = FALSE,   # Cacher le scree plot
                    nf = 131            # Nombre d'axes gardés
                    )

```
<br> </br>

## Visualisation

<br> </br>
Affichage des individus
```{r}
#plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=5,label="var",graph.type = "ggplot")
fviz_pca_ind(res.pca,
             col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     
             )
```
<br> </br>
<br> </br>
<br> </br>

## Sélection du nombre d'axes

<br> </br>
Observons l'inertie expliqué
```{r}
#head(res.pca$eig)
screeplot(res.pca, main = "Valeurs propres")
fviz_eig(res.pca)

library(factoextra)
# Valeurs propres
vp <- get_eigenvalue(res.pca)
head(vp)



```

On utilise les valeurs propres pour déterminer le nombre p' d'axes principaux à conserver après l'ACP avec la règle de Kaiser (1961)

- On prend les valeurs propre > 1, impliquant que la composante va représenté plus de variance par rapport à une seule variable d'origine 

- On prend un ensemble de valeurs propres qui ont une variance cumulée d'au moins 70% (explique au moins 70% de l'inertie)



<br> </br>
Ici on prend les 24 premières compostantes pour former nos 24 axes principaux

```{r}
# #res.pca$ind$coord
# 
# #Variables
# res.var <- get_pca_var(res.pca)
# 
# # Coordonnées
# #head(res.var$coord  )
# 
# # Contributions aux axes
# #head(res.var$contrib   )
# 
# # Qualité de représentation 
# head(res.var$cos2  )
# 
# #Isndividus
# res.ind <- get_pca_ind(res.pca)
# 
# # Coordonnées
# #head(res.ind$coord )
# 
# # Contributions aux axes
# #head(res.ind$contrib )
# 
# # Qualité de représentation
# head(res.ind$cos2  )         
# ```


```
<br> </br>


## Qualité

<br> </br>
qualité de répresentation
```{r,results='hide'}
sujetVar <- get_pca_var(res.pca)

head(sujetVar$cos2)
```
<br> </br>
Qualité par axes coloré 
```{r}
fviz_pca_var(
  res.pca,
  col.var = "cos2",
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = TRUE
)

```
<br> </br>

<br> </br>
<br> </br>

Top 5 des variables explicatives du premier axe
```{r}

#
fviz_contrib(res.pca,
             choice = "var",
             axes = 1,
             top = 5)
```

<br> </br>



Tracage d'ellypse de confiance
```{r}


fviz_pca_ind(
  res.pca,
  geom.ind = "point",
  col.ind = train$groupe,
  palette = c("#00AFBB", "#E7B800", "#FC4E07", "#9bcd9b"),
  addEllipses = TRUE,
  ellipse.type = "confidence",
  legend.title = "continents"
)
```
<br> </br>

# ACP sur jeu de test

<br> </br>

On projette les données du jeu test avec l'ACP effectué sur le jeu d'entrainement.
```{r}
library(ade4)
ACPsurTest <- suprow(res.pca, test[,2:132])$lisup

```
<br> </br>

Visualisation projection des nouveaux individus
```{r}
#plot des obs train
plot <- fviz_pca_ind(res.pca, repel = TRUE)

#plot des obs de test
fviz_add(plot, ACPsurTest, color ="purple")
```
<br> </br>
<br> </br>

On obtient une base train de 24 variables pour 79999 observations

On prépare les bases d'entrainements et de tests pour le SVM, avec et sans ACP.
```{r}
#AVEC ACP
trainACP <- as.data.frame(
 # res.pca$ind$coord[, 1:24]
  res.pca$li[, 1:24]
  )
testACP <- ACPsurTest[, 1:24]
y = train[, 134]
y.test = test[, 134]
trainACP <- cbind(trainACP, y)
testACP <- cbind(testACP, y.test)

#SANS ACP
train_SVM <- train[,-1]
test_SVM <- test[,-1]
train_SVM$churner <- NULL
test_SVM$churner <- NULL

```
<br> </br>


# SVM

<br> </br>

## Introduction

<br> </br>

SVM utilisé en tant que classifieur permet de trouver un ligne (R²), un plan (R^3), un hyperplan (R^n) qui sépare les observations en classes.

Dans le cas du SVM linaéaire binaire on séparer linéairement une Classe +1 et une Classe -1, 
Une infinité de solutions est possible pour ces problèmes,on cherche la solution qui maximise la marge et minimise les erreurs.

Cependant lors de la recherche des points supports, les outliers(valeurs abérantes) vont faire perdre toute généralisation au modèle,
pour contrer cela on accepte alors des erreurs, on change de classe les outliers.


<br> </br>

## Création SVM

<br> </br>
ajout d'une colonne temps d'execution dans tableau
2 tableaux : un ACP, l'autre sans ACP.


Fonction création d'un SVM
```{r}
svm_para <- function(type , kernel, x , y)
{
  library(e1071)
 # require(doSNOW)
  
 #  cl <- makeCluster(8, type = "SOCK") 
  # registerDoSNOW(cl)

  svmfit <- tune.svm(
    x = x,
    y = y,
    type = type,
    kernel = kernel,
    cost = c(1,10), #1:10
    gamma = c(0, 0.5), #seq(0, 0.5, by = 0.1)
    tunecontrol = tune.control(cross = 2)
  )
  
  
#  stopCluster(cl)
  
  
  return (svmfit)
}
```
les paramètres différentiant les SVM : le type de classifieur & la fonction kernel utilisé.

<br> </br>

<br> </br>

## Critères de comparaison des SVM

<br> </br>

Fonction d'évaluation d'un SVM
```{r}
Performances_SVM <- function(svmfit,type,kernel,tempsSVM,trainJeu,testJeu,collone)
{
  library(caret)
  Resultat <- list(7)
  Resultat[1] <- type
  Resultat[2] <- kernel
  Resultat[3] <- svmfit$best.parameters$cost
  Resultat[4] <- svmfit$best.parameters$gamma

  #Prédictions
  SVM_pred_training = predict(svmfit$best.model, newdata  = trainJeu[,-collone]) 
  SVM_pred_testing = predict(svmfit$best.model, newdata  = testJeu[,-collone] )

  #Matrices de confusion train et test
  MatriceConfu_SVM_train <-
    confusionMatrix(table (SVM_pred_training, trainJeu[, collone]))
  MatriceConfu_SVM_test <-
    confusionMatrix(table (SVM_pred_testing, testJeu[, collone]))
  
  #retour de résultat des matrices de confusions
  Resultat[5:6] <- MatriceConfu_SVM_train$overall[1:2]
  Resultat[7:9] <- MatriceConfu_SVM_train$byClass[c("Sensitivity","Specificity","Balanced Accuracy")]
  
  Resultat[10:11] <- MatriceConfu_SVM_test$overall[1:2]
  Resultat[12:14] <-MatriceConfu_SVM_test$byClass[c("Sensitivity","Specificity","Balanced Accuracy")]

  Resultat[15] <- svmfit$best.model$tot.nSV
  Resultat[16] <- tempsSVM
  
  return (Resultat)
}
```
On créé une liste contenant les différentes mesures de performances pour le jeu d'entrainement Et le jeu de test.

Kappa, Sensitivity, Specificity et Balanced Accuracy sont très utiles dans notre cas puisque la proportion des classes est très différentes, ces paramètres nous permettent de mettre en perspective notre Accuracy.

Idée : choisir la spécificité (=/ de sensibilité) pour notre cas ?

http://john-uebersax.com/stat/kappa.htm

<br> </br>


Création d'un tableau pour afficher les performances des différents SVM selon les paramètres
```{r}
N <- 8  # nombre de lignes

Tableau_Resultat_SVM_avec_ACP <- data.frame(
  type = rep("", N),
  kernel = rep("", N),
  BestCost = rep(NA, N),
  BestGamma = rep(NA, N),
  
  Accuracy_Training = rep(NA, N),
  Kappa__Training = rep(NA, N),
  Sensitivity_Training = rep(NA, N),
  Specificity_Training = rep(NA, N),
  Balanced_Accuracy_Training = rep(NA, N),
  
  Accuracy_Testing = rep(NA, N),
  Kappa__Testing = rep(NA, N),
  Sensitivity_Testing = rep(NA, N),
  Specificity_Testing = rep(NA, N),
  Balanced_Accuracy_Testing = rep(NA, N),
  
  
  Nombre_De_Vecteur_Support = rep(NA, N),
    Temps_D_Execution_En_Seconde = rep(NA, N),
  
  stringsAsFactors = FALSE
)

Tableau_Resultat_SVM_sans_ACP <- data.frame(
  type = rep("", N),
  kernel = rep("", N),
  BestCost = rep(NA, N),
  BestGamma = rep(NA, N),
  
  Accuracy_Training = rep(NA, N),
  Kappa__Training = rep(NA, N),
  Sensitivity_Training = rep(NA, N),
  Specificity_Training = rep(NA, N),
  Balanced_Accuracy_Training = rep(NA, N),
  
  Accuracy_Testing = rep(NA, N),
  Kappa__Testing = rep(NA, N),
  Sensitivity_Testing = rep(NA, N),
  Specificity_Testing = rep(NA, N),
  Balanced_Accuracy_Testing = rep(NA, N),
  
  
  Nombre_De_Vecteur_Support = rep(NA, N),
    Temps_D_Execution_En_Seconde = rep(NA, N),
  
  stringsAsFactors = FALSE
)



#svmfit_normal   = svm(formula = y~.,
#                data = trainACP,
#              type = 'C-classification',
#          kernel = 'linear')


```


<br> </br>


## Comparaison des SVM


<br> </br>
On créé deux vecteurs, l'un contenant tout les types de classifieur & l'autre contenant les fonctions kernel possible.

On test toute les combinaisons possibles en appelant notre fonction de création de SVM, puis on évalue le modèle avec la fonction d'évaluation de SVM. On sauvegarde ces performances dans le tableau, et si le modèle a une meilleure Spécificité que le meilleur actuel, on affecte ce modèle au meilleur.
```{r}
#type
type_liste <- c("C-classification",
                "nu-classification",
                "one-classification"
                #      "eps-regression",
                #     "nu-regression"
                )
                
#kernel
kernel_liste <-
  c("linear", "polynomial", "radial", "sigmoid")


#variables
i = 1
meilleur_Accuracy_ACP = 0
meilleur_Accuracy_sans_ACP = 0

#boucles
for (x in seq_along(type_liste)) {
  for (y in seq_along(kernel_liste)) {
    print("----------------")
    print(type_liste[x])
    print(kernel_liste[y])
    
    #nu classification uniquement compatible avec un kernel linéaire
    if (!(x == 2 && y != 1)) {
      #one classification pas compatible avec kernel polynomial
      if (!(x == 3 && y == 2)) {
        
        
        #On test toute les combinaisons possibles en appelant notre fonction de création de SVM
        #et on récupère le temps de calcul effectué
        debut <- Sys.time()
        svm_avec_ACP <- svm_para(type_liste[x], kernel_liste[y],trainACP[,-25],trainACP[, 25])
        TempsSVM <- Sys.time() - debut
        
        #On sauvegarde ces performances dans le tableau
        Tableau_Resultat_SVM_avec_ACP[i, ] <-
          Performances_SVM(svm_avec_ACP, type_liste[x], kernel_liste[y],TempsSVM,trainACP,testACP,25)
        
        #si le modèle a une meilleure Balanced Accuracy que le meilleur actuel, on affecte ce modèle au meilleur.
       if( Tableau_Resultat_SVM_avec_ACP[i,13 ] > meilleur_Accuracy_ACP){
         meilleur_svm_avec_ACP <- svm_avec_ACP
         meilleur_Accuracy_ACP <- Tableau_Resultat_SVM_avec_ACP[i,13 ]
       }
        # One classification avec kernel radial et sigmoid a un temps de calcul très long (trop) avec la base sans ACP
        if(!(x==3 && y > 2)){
       #Exactement la même chose, mais sans l'ACP effectuée sur les jeux de données
       debut <- Sys.time()
       svm_sans_ACP <- svm_para(type_liste[x], kernel_liste[y],train_SVM[,-132],train_SVM[, 132])
       TempsSVM <- Sys.time() - debut
       Tableau_Resultat_SVM_sans_ACP[i, ] <-
         Performances_SVM(svm_sans_ACP, type_liste[x], kernel_liste[y],TempsSVM,train_SVM,test_SVM,132)
       if( Tableau_Resultat_SVM_sans_ACP[i,13 ] > meilleur_Accuracy_sans_ACP){
        meilleur_svm_sans_ACP <- svm_sans_ACP
        meilleur_Accuracy_sans_ACP <- Tableau_Resultat_SVM_sans_ACP[i,13 ]
       }
        
        }
       
        i <- i + 1
      }
    }
    
  }
}

```

<br> </br>
<br> </br>

On obtient alors un tableau avec les performances de chaque modèle et le modèle ayant la Balanced Accuracy la plus haute.
```{r}

#performance des modèles Avec ACP
Tableau_Resultat_SVM_avec_ACP

#performance des modèles Sans ACP
Tableau_Resultat_SVM_sans_ACP

#library(tidyverse)
# result1  <- bind_cols(Tableau_Resultat_SVM[,1:2] , Tableau_Resultat_SVM[,3:4], Tableau_Resultat_SVM[,15])
# result2  <- bind_cols(Tableau_Resultat_SVM[,1:2] , Tableau_Resultat_SVM[,5:9])
# result3  <- bind_cols(Tableau_Resultat_SVM[,1:2] , Tableau_Resultat_SVM[,10:14])
# 
# result1
# result2
# result3

#meilleur modèle avec ACP
#meilleur_svm_avec_ACP

#meilleur modèle sans ACP
#meilleur_svm_sans_ACP
```
## Conclusion

lorem ipsum


# Réseau de neurones

<br> </br>

## Perceptron simple

Constantes de notre réseau de neurone nnet.
Poids aléatoires initiaux sur [-valInitWgt, valInitWgt]. 

Valeur d'environ 0,5 sauf si les entrées sont grandes, auquel cas elle doit être choisie de manière à ce quevalInitWgt*max(|x|) soit environ 1.
```{r}
library(nnet)

valInitWgt=0.6

# neurones linéaires (TRUE) ou sigmoïdes (FALSE) sur la couche de sortie.
linearOutput=FALSE

#   Nombre de neurones sur la couche cachée
nbNeurons=500

# Nombre maximal d'itérations pendant la phase d'apprentissage
maxLearnIter=1000

```
<br> </br>

Catégorisons en utilisant un réseau neuronal multicouche avec 24 entrées et 1 sortie.
# Vecteur d'entrée : 24 premières dimensions de notre ACP
# Vecteur de sortie : 1 pour churner, 0 sinon.
```{r}
x = trainACP[,-25]
y = matrix(0, nr = length(trainACP[, 1]), nc = 1)


for (i in 1:length(trainACP[, 1])) {
  if (trainACP[i, 25] == TRUE) {
    y[i, ] = 1
  }
  else {
    y[i, ] = 0
  }
  
}

```



    
Size décrit le nombre de nœuds qui seront utilisés dans la couche cachée, dans ce cas 500 nœuds sont utilisés. Decay illustre la vitesse de décroissance de la descente de gradient. Maxit est l'itération maximale à effectuer, dans ce cas, l'itération maximale à effectuer est de 1000 itérations.

```{r}
#faire varier size, decay

rna01 <- nnet(
  x, 
  y, 
  MaxNWts = 10^5,
  size=nbNeurons, 
  linout=linearOutput,
  rang=valInitWgt,
  decay=0,
  maxit=maxLearnIter,
  Hess=TRUE)
```

```{r}
# Utiliser un réseau neuronal pour prédire les catégories à partir des entrées
y_rna<- predict(rna01,x)
```

```{r}


clust = matrix(FALSE, nr = length(y_rna[, 1]), nc = 1)


for (i in 1:length(y_rna[, 1])) {
  index = which.max(y_rna[i,])
  if (y_rna[i,] == 1) {
    clust[i, 1] = TRUE
  }
  else {
    clust[i, 1] = FALSE
  }
}


trainACP2 = trainACP
trainACP2$Cluster = clust
write.table(trainACP2, file = "trainACP.txt", sep = " ") # save result in text file


```


Prédiction pour les lignes de tests
```{r}
# 
y_rna2 <- predict(rna01, testACP[,-25])

testACP2 = testACP
clust2 = matrix(FALSE, nr = length(y_rna2[, 1]), nc = 1)


for (i in 1:length(y_rna2[, 1])) {
  if (y_rna[i, ] == 1) {
    clust2[i, 1] = TRUE
  }
  else {
    clust2[i, 1] = FALSE
  }
}
testACP2$Cluster = clust2


```

```{r}
#matrice de confusion
table(testACP2[,25])
table(testACP2[,26])
table(testACP2[,25:26])

#tables de contingences
table(testACP2[,25])
table(testACP2[,26])
tres=table(testACP2[,25:26])
prop.table(tres,1)
```

## Paramètres à optimiser pour un perceptron

Tune nnet
```{r}
Qlist <- 1:10
iter <- 10

res_tune <- tune.nnet(x, y, tune.control(nrepeat = iter), size = Qlist,
                      linout = TRUE)
```

Visualisation évolution performance
```{r}
plot(Qlist, res_tune$performances$error, main = "", xlab = "Q", ylab = "erreur",
     type = "b")
```

Affichage performance et meilleur modèle
```{r}
res_tune$best.parameters
res_tune$best.performance
res_tune$best.model
```




## Réseau de neurones multicouches
```{r}
library(keras)



```




