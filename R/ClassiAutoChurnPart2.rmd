---
title: |
  ![](images/svm.png){width=20%} 
  Partie 2 sur R
author: 
- Clovis Deletre
- Charles Vitry
date:
output:
  rmarkdown::html_document:
    theme: cerulean
    number_sections: no
    toc: yes
    toc_depth: 5
    toc_float: true
---
<style type="text/css">

body{ /* Normal  */
      font-size: 20px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 55px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 38px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 28px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 35px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install for export in pdf file
#tinytex::install_tinytex()
```
<br> </br>





<br> </br>


# Introduction
<br> </br>
Import des deux jeux de données | format csv 
```{r}
train <-
  read.csv(file = "sample_churner_train_data_set.csv", header = TRUE, sep = ",")
#head(train)

test <- read.csv(file = "sample_churner_test_data_set.csv", header = TRUE, sep =
                   ",")
#head(test)

#Suppression d'une partie des lignes pour faire les tests d'éxécution & génération de l'html de façon plus rapide (on garde 10%)
lignes_sup_train <- sample(1:nrow(train), nrow(train)*0.90)
lignes_sup_test <- sample(1:nrow(test), nrow(test)*0.90)
train <- train[-lignes_sup_train,]
test <- test[-lignes_sup_test,]



```
<br> </br>
Création variables
```{r}
train$groupe <- ifelse(train$churner == 1 , TRUE, FALSE)
test$groupe <- ifelse(test$churner == 1 , TRUE, FALSE)
```
<br> </br>    


# ACP sur Jeu d'entrainement
<br> </br>

Le paramètre Scale.uni permet de choisir de réduire ou non les variables.

ncp est le nombre de dimensions à garder dans les résultats

Tandis que graph est le choix de faire apparaître les graphiques ou non 
```{r,fig.show='hide',warning=FALSE}
library(FactoMineR)
library("factoextra")

#Test d'une ACP en prenant churner en variable qualitative
res.pca = PCA(
  train[, 2:133],
  scale.unit = TRUE,
  quali.sup = 132 ,
  ncp = 131,
  graph = T
)

```
<br> </br>

## Création de l'ACP

```{r,fig.show='hide',warning=FALSE}
library(FactoMineR)
library("factoextra")

#ACP en excluant la collone d'identification de l'individu ainsi que la variable à expliquée.
#res.pca = PCA(train[,2:132], scale.unit=TRUE, ncp=131, graph=T)


#mauvaise méthode, faire l'ACP sur les données à tester n'a aucun sens, 
#en effet, pour estimer le prix d'un contrat, on ne peut pas attendre que 5000 clients attendent l'estimation de leur contrat
#il faut projeter les nouvelles données avec l'ACP effectué sur le jeu d'entrainement.
#res.pca.test = PCA(test[,2:132], scale.unit=TRUE, ncp=131, graph=T)


library(ade4)
res.pca <- dudi.pca(train[,2:132],
                    scannf = FALSE,   # Cacher le scree plot
                    nf = 131            # Nombre d'axes gardés
                    )

```
<br> </br>

## Visualisation

<br> </br>
Affichage des individus
```{r}
#plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=5,label="var",graph.type = "ggplot")
fviz_pca_ind(res.pca,
             col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     
             )
```
<br> </br>
<br> </br>
<br> </br>

## Sélection du nombre d'axes

<br> </br>
Observons l'inertie expliqué
```{r}
#head(res.pca$eig)
screeplot(res.pca, main = "Valeurs propres")
fviz_eig(res.pca)

library(factoextra)
# Valeurs propres
vp <- get_eigenvalue(res.pca)
head(vp)




```

On utilise les valeurs propres pour déterminer le nombre p' d'axes principaux à conserver après l'ACP avec la règle de Kaiser (1961)

- On prend les valeurs propre > 1, impliquant que la composante va représenté plus de variance par rapport à une seule variable d'origine 

- On prend un ensemble de valeurs propres qui ont une variance cumulée d'au moins 70% (explique au moins 70% de l'inertie)



<br> </br>
Ici on prend les 24 premières composantes pour former nos 24 axes principaux

```{r}
# #res.pca$ind$coord
# 
# #Variables
# res.var <- get_pca_var(res.pca)
# 
# # Coordonnées
# #head(res.var$coord  )
# 
# # Contributions aux axes
# #head(res.var$contrib   )
# 
# # Qualité de représentation 
# head(res.var$cos2  )
# 
# #Isndividus
# res.ind <- get_pca_ind(res.pca)
# 
# # Coordonnées
# #head(res.ind$coord )
# 
# # Contributions aux axes
# #head(res.ind$contrib )
# 
# # Qualité de représentation
# head(res.ind$cos2  )         
# ```


```
<br> </br>


## Qualité

<br> </br>
Qualité de représentation
```{r,results='hide'}
sujetVar <- get_pca_var(res.pca)

head(sujetVar$cos2)
```
<br> </br>
Qualité par axes coloré 
```{r}
fviz_pca_var(
  res.pca,
  col.var = "cos2",
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = TRUE
)

```
<br> </br>

<br> </br>
<br> </br>

Top 5 des variables explicatives du premier axe
```{r}

#
fviz_contrib(res.pca,
             choice = "var",
             axes = 1,
             top = 5)
```

<br> </br>



Traçage d’ellipse de confiance
```{r}


fviz_pca_ind(
  res.pca,
  geom.ind = "point",
  col.ind = train$groupe,
  palette = c("#00AFBB", "#E7B800", "#FC4E07", "#9bcd9b"),
  addEllipses = TRUE,
  ellipse.type = "confidence",
  legend.title = "Churner"
)
```
<br> </br>
<br> </br>
<br> </br>

# ACP sur jeu de test

<br> </br>

On projette les données du jeu test avec l'ACP effectué sur le jeu d’entraînement.
```{r}
library(ade4)
ACPsurTest <- suprow(res.pca, test[,2:132])$lisup

```
<br> </br>

Visualisation projection des nouveaux individus
```{r}
#plot des obs train
plot <- fviz_pca_ind(res.pca, repel = TRUE)

#plot des obs de test
fviz_add(plot, ACPsurTest, color ="purple")
```
<br> </br>
<br> </br>

On obtient une base train de 24 variables pour 79999 observations

On prépare les bases d’entraînements et de tests pour le SVM, avec et sans ACP.
```{r}
#AVEC ACP
trainACP <- as.data.frame(
 # res.pca$ind$coord[, 1:24]
  res.pca$li[, 1:24]
  )
testACP <- ACPsurTest[, 1:24]
y = train[, 134]
y.test = test[, 134]
trainACP <- cbind(trainACP, y)
testACP <- cbind(testACP, y.test)

#SANS ACP
train_SVM <- train[,-1]
test_SVM <- test[,-1]
train_SVM$churner <- NULL
test_SVM$churner <- NULL
# groupe_train <- train_SVM$groupe
# groupe_test <- test_SVM$groupe

```
<br> </br>

On centre-réduit les données sur lequels nous n'avons pas effectuer d'ACP
```{r warning=FALSE,error=TRUE}
# ## Calculs de la moyenne et de l'écart type pour chaque collonne du jeu de test
# train_SVM_Moyenne <- apply(train_SVM,2,mean)
# train_SVM_EcartType <- apply(train_SVM,2,sd)
# 
# ## On centre réduit avec la moyenne et l'écart type du jeu d'entrainement
# train_SVM <- sweep(sweep(train_SVM, 2L, train_SVM_Moyenne), 2, train_SVM_EcartType, "/")
# test_SVM <- sweep(sweep(test_SVM, 2L, train_SVM_Moyenne), 2, train_SVM_EcartType, "/")
#La variable à prédire a été aussi centrée réduite, corrigons cela
# train_SVM$groupe <- groupe_train
# test_SVM$groupe <- groupe_test

#Utilisation plus facile avec la fonction predict
Centree_reduit <- preProcess(train_SVM[,-132])
test_SVM[,-132] <- predict(Centree_reduit, test_SVM[,-132])
train_SVM[,-132] <- predict(Centree_reduit, train_SVM[,-132])

```


<br> </br>

# SVM

<br> </br>

## Introduction

<br> </br>

Le modèle SVM utilisé en tant que classificateur permet de trouver une ligne (R²), un plan (R^3), un hyperplan (R^n) qui sépare les observations en classes.

Dans le cas du SVM linéaire binaire on sépare linéairement une Classe +1 et une Classe -1, 
Une infinité de solutions est possible pour ces problèmes,on cherche la solution qui maximise la marge et minimise les erreurs.

Cependant lors de la recherche des points supports, les outliers(valeurs aberrantes) vont faire perdre toute généralisation au modèle,
pour contrer cela on accepte alors un taux d'erreur.


<br> </br>

## Création SVM

<br> </br>

Fonction création d'un SVM
```{r}
svm_para <- function(type , kernel, x , y)
{
  library(e1071)
 # require(doSNOW)
  
 #  cl <- makeCluster(8, type = "SOCK") 
  # registerDoSNOW(cl)

 
  
 svmfit <- tune.svm(
    x = x,
    y = y,
    type = type,
    kernel = kernel,
    cost = 100000 ,#c(10,100000), #1:10
    gamma =10,# c(0,1), #seq(0, 0.5, by = 0.1)
    tunecontrol = tune.control(cross = 2),
 # preProc = c("center","scale"), Si l'on souhaite réaliser le SVM sur des données centrée réduite.
  metric = "Specificity"
  )
  
  
#  stopCluster(cl)
  
  
  return (svmfit)
}
```
les paramètres différenciant les SVM : le type de classificateur & la fonction kernel utilisé.

<br> </br>

<br> </br>

## Critères de comparaison des SVM

<br> </br>

Fonction d'évaluation d'un SVM
```{r}
Performances_SVM <- function(svmfit,type,kernel,tempsSVM,trainJeu,testJeu,collone)
{
  library(caret)
  Resultat <- list(7)
  Resultat[1] <- type
  Resultat[2] <- kernel
  Resultat[3] <- svmfit$best.parameters$cost
  Resultat[4] <- svmfit$best.parameters$gamma

  #Prédictions
  SVM_pred_training = predict(svmfit$best.model, newdata  = trainJeu[,-collone]) 
  SVM_pred_testing = predict(svmfit$best.model, newdata  = testJeu[,-collone] )

  #Matrices de confusion train et test
  MatriceConfu_SVM_train <-
    confusionMatrix(table (SVM_pred_training, trainJeu[, collone]))
  MatriceConfu_SVM_test <-
    confusionMatrix(table (SVM_pred_testing, testJeu[, collone]))
  
  #retour de résultat des matrices de confusions
  Resultat[5:6] <- MatriceConfu_SVM_train$overall[1:2]
  Resultat[7:9] <- MatriceConfu_SVM_train$byClass[c("Sensitivity","Specificity","Balanced Accuracy")]
  
  Resultat[10:11] <- MatriceConfu_SVM_test$overall[1:2]
  Resultat[12:14] <-MatriceConfu_SVM_test$byClass[c("Sensitivity","Specificity","Balanced Accuracy")]

  Resultat[15] <- svmfit$best.model$tot.nSV
  Resultat[16] <- tempsSVM
  
  return (Resultat)
}
```
On créé une liste contenant les différentes mesures de performances pour le jeu d’entraînement Et le jeu de test.

Kappa, Sensitivity, Specificity et Balanced Accuracy sont très utiles dans notre cas puisque la proportion des classes est très différentes, ces paramètres nous permettent de mettre en perspective notre Accuracy.

<br> </br>


Création d'un tableau pour afficher les performances des différents SVM selon les paramètres
```{r}
N <- 8  # nombre de lignes

Tableau_Resultat_SVM_avec_ACP <- data.frame(
  type = rep("", N),
  kernel = rep("", N),
  BestCost = rep(NA, N),
  BestGamma = rep(NA, N),
  
  Accuracy_Training = rep(NA, N),
  Kappa__Training = rep(NA, N),
  Sensitivity_Training = rep(NA, N),
  Specificity_Training = rep(NA, N),
  Balanced_Accuracy_Training = rep(NA, N),
  
  Accuracy_Testing = rep(NA, N),
  Kappa__Testing = rep(NA, N),
  Sensitivity_Testing = rep(NA, N),
  Specificity_Testing = rep(NA, N),
  Balanced_Accuracy_Testing = rep(NA, N),
  
  
  Nombre_De_Vecteur_Support = rep(NA, N),
    Temps_D_Execution_En_Seconde = rep(NA, N),
  
  stringsAsFactors = FALSE
)

Tableau_Resultat_SVM_sans_ACP <- data.frame(
  type = rep("", N),
  kernel = rep("", N),
  BestCost = rep(NA, N),
  BestGamma = rep(NA, N),
  
  Accuracy_Training = rep(NA, N),
  Kappa__Training = rep(NA, N),
  Sensitivity_Training = rep(NA, N),
  Specificity_Training = rep(NA, N),
  Balanced_Accuracy_Training = rep(NA, N),
  
  Accuracy_Testing = rep(NA, N),
  Kappa__Testing = rep(NA, N),
  Sensitivity_Testing = rep(NA, N),
  Specificity_Testing = rep(NA, N),
  Balanced_Accuracy_Testing = rep(NA, N),
  
  
  Nombre_De_Vecteur_Support = rep(NA, N),
    Temps_D_Execution_En_Seconde = rep(NA, N),
  
  stringsAsFactors = FALSE
)



#svmfit_normal   = svm(formula = y~.,
#                data = trainACP,
#              type = 'C-classification',
#          kernel = 'linear')


```


<br> </br>


## Comparaison des SVM


<br> </br>
On créé deux vecteurs, l'un contenant tout les types de classificateur & l'autre contenant les fonctions kernel possible.

On test toute les combinaisons possibles en appelant notre fonction de création de SVM, puis on évalue le modèle avec la fonction d'évaluation de SVM. On sauvegarde ces performances dans le tableau, et si le modèle a une meilleure Spécificité que le meilleur actuel, on affecte ce modèle au meilleur.
```{r warning=FALSE}
#type
type_liste <- c("C-classification"
                #,
            #    "nu-classification",
               # "one-classification"
                #      "eps-regression",
                #     "nu-regression"
                )
                
#kernel
kernel_liste <-
  c("linear", "polynomial", "radial", "sigmoid")


#variables
i = 1
meilleur_Accuracy_ACP = 0
meilleur_Accuracy_sans_ACP = 0

#boucles
for (x in seq_along(type_liste)) {
  for (y in seq_along(kernel_liste)) {
    print("----------------")
    print(type_liste[x])
    print(kernel_liste[y])
    
    #nu classification uniquement compatible avec un kernel linéaire
    if (!(x == 2 && y != 1)) {
      #one classification pas compatible avec kernel polynomial
      if (!(x == 3 && y == 2)) {
        
        
        #On test toute les combinaisons possibles en appelant notre fonction de création de SVM
        #et on récupère le temps de calcul effectué
        debut <- Sys.time()
        svm_avec_ACP <- svm_para(type_liste[x], kernel_liste[y],trainACP[,-25],trainACP[, 25])
        TempsSVM <- Sys.time() - debut
        
        #On sauvegarde ces performances dans le tableau
        Tableau_Resultat_SVM_avec_ACP[i, ] <-
          Performances_SVM(svm_avec_ACP, type_liste[x], kernel_liste[y],TempsSVM,trainACP,testACP,25)
        
        #si le modèle a une meilleure Balanced Accuracy que le meilleur actuel, on affecte ce modèle au meilleur.
       if( Tableau_Resultat_SVM_avec_ACP[i,13 ] > meilleur_Accuracy_ACP){
         meilleur_svm_avec_ACP <- svm_avec_ACP
         meilleur_Accuracy_ACP <- Tableau_Resultat_SVM_avec_ACP[i,13 ]
       }
        print("Partie ACP terminé")
        # One classification avec kernel radial et sigmoid a un temps de calcul très long (trop) avec la base sans ACP
        if(!(x==3 && y > 2)){
       #Exactement la même chose, mais sans l'ACP effectuée sur les jeux de données
       debut <- Sys.time()
       svm_sans_ACP <- svm_para(type_liste[x], kernel_liste[y],train_SVM[,-132],train_SVM[, 132])
       TempsSVM <- Sys.time() - debut
       Tableau_Resultat_SVM_sans_ACP[i, ] <-
         Performances_SVM(svm_sans_ACP, type_liste[x], kernel_liste[y],TempsSVM,train_SVM,test_SVM,132)
       if( Tableau_Resultat_SVM_sans_ACP[i,13 ] > meilleur_Accuracy_sans_ACP){
        meilleur_svm_sans_ACP <- svm_sans_ACP
        meilleur_Accuracy_sans_ACP <- Tableau_Resultat_SVM_sans_ACP[i,13 ]
       }
        
        }
       
        i <- i + 1
      }
    }
    
  }
}

```

<br> </br>
<br> </br>

On obtient alors un tableau avec les performances de chaque modèle et le modèle ayant la Balanced Accuracy la plus haute.
```{r}

#performance des modèles Avec ACP
Tableau_Resultat_SVM_avec_ACP

#performance des modèles Sans ACP
Tableau_Resultat_SVM_sans_ACP


#Sauvegarde des résultats
write.csv(Tableau_Resultat_SVM_avec_ACP, file = "Resultat_SVM_Avec_ACP.csv")
write.csv(Tableau_Resultat_SVM_sans_ACP, file = "Resultat_SVM_Sans_ACP.csv")


#library(tidyverse)
# result1  <- bind_cols(Tableau_Resultat_SVM[,1:2] , Tableau_Resultat_SVM[,3:4], Tableau_Resultat_SVM[,15])
# result2  <- bind_cols(Tableau_Resultat_SVM[,1:2] , Tableau_Resultat_SVM[,5:9])
# result3  <- bind_cols(Tableau_Resultat_SVM[,1:2] , Tableau_Resultat_SVM[,10:14])
# 
# result1
# result2
# result3

#meilleur modèle avec ACP
#meilleur_svm_avec_ACP

#meilleur modèle sans ACP
#meilleur_svm_sans_ACP
```
## Conclusion

Après test de nombreuses combinaisons de nos paramètres nous avons pu trouver de bon modèles, nous remarquons que le temps de calculs est plus faible avec la version ACP (presque 15 fois plus rapide), les résultats sont similaires. 

Nous pouvons obtenir des résultats équivalents que nos données soit bruts, Centrée-réduite, ou Composantes principales, mais la différence de temps d'éxécution donne l'avantage aux composantes principales. 

Nous avons pu obtenir un modèle avec 0.9 de sensibilité et 0.5 de spécificité pour chacun des jeux de données, cela permettrait de bien prédire les non-churner et de toucher la moitié des churners. Il est possible de le mettre en production car il prédit correctement les non-churner, mais la prédiction d'uniquement 50% des churners n'est pas suffisante.

Ces métrics ont été obtenu sur le jeu de test de 15 0000 lignes grâce aux hyperparamètres suivants :

* ACP : C-Classification, polynomial, cost : 10 000, gamma : 20

* Non-ACP C-Classification, polynomial, cost : 100000, gamma : 10

# Perceptrons

<br> </br>

## Perceptron simple

Préparation jeu de donnée pour le perceptron.

Catégorisons en utilisant un réseau neuronal multicouche avec 24 entrées et 1 sortie. (pour le jeu ACP).

* Vecteur d'entrée : 24 premières dimensions de notre ACP
* Vecteur de sortie : 1 pour churner, 0 sinon.
```{r}
x_ACP = trainACP[,-25]
y_ACP = matrix(0, nr = length(trainACP[, 1]), nc = 1)

x_sans_ACP  = train_SVM[,-25]
y_sans_ACP  = matrix(0, nr = length(train_SVM[, 1]), nc = 1)


for (i in 1:length(trainACP[, 1])) {
  if (trainACP[i, 25] == TRUE) {
    y_ACP[i, ] = 1
  }
  else {
    y_ACP[i, ] = 0
  }
  
}

for (i in 1:length(train_SVM[, 1])) {
  if (train_SVM[i, 25] == TRUE) {
    y_sans_ACP[i, ] = 1
  }
  else {
    y_sans_ACP[i, ] = 0
  }
  
}

```


<br> </br>

Les constantes de notre réseau de neurone nnet.
Poids aléatoires initiaux sur [-valInitWgt, valInitWgt]. 

On a choisit une valeur de 0,6 car les entrées sont moyennes, sinon on aurait choisit tel que valInitWgt*max(|x|) soit environ égale à 1.
```{r}
library(nnet)

valInitWgt=0.6

# neurones linéaires (TRUE) ou sigmoïdes (FALSE) sur la couche de sortie.
linearOutput=FALSE

#   Nombre de neurones sur la couche cachée
nbNeuronsList =  3 #c(1,3,5,10,20)

#la fonction objectif minimisée lors de l’apprentissage R(w)+λ∥w∥², faisons varier λ le paramètre de régularisation
reguList <- 10^-5 #c(10^-7,10^-5,10^-3, 0.1)



# Nombre maximal d'itérations pendant la phase d'apprentissage
maxLearnIter=500

```
<br> </br>


    
Size décrit le nombre de nœuds qui seront utilisés dans la couche cachée, dans ce cas 3 nœuds sont utilisés. Decay illustre la vitesse de décroissance de la descente de gradient. Maxit est l'itération maximale à effectuer, dans ce cas, l'itération maximale à effectuer est de 1000 itérations.

Nous souhaitons observer l'influence du nombre de neurones sur la couche caché

```{r}

# ResultatsPerceptron <- function(x,y,nbNeuronsList){
# 
# 
# for (Size in nbNeuronsList) {
#   MeilleurScore <- 10 ^ 5
#   
#   
#   
#   rna01 <- nnet(
#     x,
#     y,
#     MaxNWts = 10 ^ 5,
#     size = nbNeurons,
#     linout = linearOutput,
#     rang = valInitWgt,
#     decay = 0,
#     maxit = maxLearnIter,
#     Hess = TRUE
#   )
#   
#   
#   print("===========================")
#   print("Erreur Perceptron Taille ")
#   print(Size)
#   print(rna01$residuals ^ 2)
#   
#   
#   
#   resultatsPerceptron[[Size]] <- rna01
# }
# 
#   return(resultatsPerceptron)
#   
# }
# 
# 
# 
# Perceptron_Avec_ACP <- ResultatsPerceptron(x_ACP,y_ACP,nbNeuronsList )
# Perceptron_Sans_ACP<- ResultatsPerceptron(x_sans_ACP,y_sans_ACP,nbNeuronsList )


```
<br> </br>
On test chaque size de la liste nbNeurons et chaque paramètre de régularisation
```{r}

library(e1071)

Perceptron_Avec_ACP <-
  tune.nnet(
    x_ACP,
    y_ACP,
    linout = linearOutput,
    size = nbNeuronsList,
    decay = reguList,
    MaxNWts = 10 ^ 5,
     metric = "Specificity"
  )


Perceptron_Sans_ACP <-
  tune.nnet(
    x_sans_ACP,
    y_sans_ACP,
    size = nbNeuronsList,
     decay = reguList,
    MaxNWts = 10 ^ 5,
linout = linearOutput,
 metric = "Specificity"
  )

```
<br> </br>

Affichage des modèles retenus
```{r}
Perceptron_Avec_ACP$best.parameters
Perceptron_Sans_ACP$best.parameters


```
Le meilleur modèle possède les paramètres :  

3 neurones & 1e-05 de decay.

<br> </br>

Utiliser un réseau neuronal pour prédire les catégories à partir des entrées
```{r}
#ACP

y_rna_ACP<- predict(Perceptron_Avec_ACP$best.model,x_ACP)
clust_ACP = matrix(FALSE, nr = length(y_rna_ACP[, 1]), nc = 1)

for (i in 1:length(y_rna_ACP[, 1])) {
  index = which.max(y_rna_ACP[i,])
  if (y_rna_ACP[i,] == 1) {
    clust_ACP[i, 1] = TRUE
  }
  else {
    clust_ACP[i, 1] = FALSE
  }
}
trainACP2 = trainACP
trainACP2$Cluster = clust_ACP


#Sans ACP
y_rna_SANS<- predict(Perceptron_Sans_ACP$best.model,x_sans_ACP)
clust_SANS = matrix(FALSE, nr = length(y_rna_SANS[, 1]), nc = 1)

for (i in 1:length(y_rna_SANS[, 1])) {
  index = which.max(y_rna_SANS[i,])
  if (y_rna_SANS[i,] == 1) {
    clust_SANS[i, 1] = TRUE
  }
  else {
    clust_SANS[i, 1] = FALSE
  }
}
train_SVM2 = train_SVM
train_SVM2$Cluster = clust_SANS

#write.table(trainACP2, file = "trainACP.txt", sep = " ") # save result in text file


```
<br> </br>

Prédiction sur le jeu d’entraînement
```{r}
#ACP
print("Prédiction avec ACP")
#matrice de confusion
table(trainACP2[,25])
table(trainACP2[,26])
table(trainACP2[,25:26])

#tables de contingences
table(trainACP2[,25])
table(trainACP2[,26])
tres=table(trainACP2[,25:26])
prop.table(tres,1)

#Sans ACP
print("Prédiction sans ACP")
#matrice de confusion
table(train_SVM2[,132])
table(train_SVM2[,133])
table(train_SVM2[,132:133])

#tables de contingences
table(train_SVM2[,132])
table(train_SVM2[,133])
tres=table(train_SVM2[,132:133])
prop.table(tres,1)




```
<br> </br>

Prédiction pour les lignes de tests
```{r}
#ACP

y_rna_ACP<- predict(Perceptron_Avec_ACP$best.model,testACP[,-25])
clust_ACP = matrix(FALSE, nr = length(y_rna_ACP[, 1]), nc = 1)

for (i in 1:length(y_rna_ACP[, 1])) {
  index = which.max(y_rna_ACP[i,])
  if (y_rna_ACP[i,] == 1) {
    clust_ACP[i, 1] = TRUE
  }
  else {
    clust_ACP[i, 1] = FALSE
  }
}
testACP2 = testACP
testACP2$Cluster = clust_ACP


#Sans ACP
y_rna_SANS<- predict(Perceptron_Sans_ACP$best.model,test_SVM[,-132])
clust_SANS = matrix(FALSE, nr = length(y_rna_SANS[, 1]), nc = 1)

for (i in 1:length(y_rna_SANS[, 1])) {
  index = which.max(y_rna_SANS[i,])
  if (y_rna_SANS[i,] == 1) {
    clust_SANS[i, 1] = TRUE
  }
  else {
    clust_SANS[i, 1] = FALSE
  }
}
test_SVM2 = test_SVM
test_SVM2$Cluster = clust_SANS




```

<br> </br>

```{r}
#ACP

#matrice de confusion
table(testACP2[,25])
table(testACP2[,26])
table(testACP2[,25:26])

#tables de contingences
table(testACP2[,25])
table(testACP2[,26])
tres=table(testACP2[,25:26])
prop.table(tres,1)

#Sans ACP

#matrice de confusion
table(test_SVM2[,132])
table(test_SVM2[,133])
table(test_SVM2[,132:133])

#tables de contingences
table(test_SVM2[,132])
table(test_SVM2[,133])
tres=table(test_SVM2[,132:133])
prop.table(tres,1) #[,1]
```
Les perceptrons obtiennent des résultats peu convaincants, la plupart prédisent l’entiéreté des individus comme non-churner, tandis que le meilleur ne prédit qu'à 70% de sensibilité et 65% de spécificité. On remarque de meilleurs résultat avec les jeux de données ACP et les données bruts, tandis que les modèles réalisés avec les données centrée-réduite ont systématiquement une spécificité nulle.




